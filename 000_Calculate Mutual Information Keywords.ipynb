{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf14414",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset path\"\n",
    "filename = 'dataset csv file *.csv'\n",
    "awlistFile = 'anchor words file *.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "%matplotlib inline\n",
    "\n",
    "import MeCab\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist \n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab \n",
    "from konlpy.tag import *\n",
    "\n",
    "mc = Mecab(dicpath='The path of the MeCab-ko dictionary.') # The path of the MeCab-ko dictionary.\n",
    "\n",
    "stwDF = pd.read_excel(path+awlistFile, sheet_name='stopwords')\n",
    "stwlist = stwDF.space.to_list()\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "    \n",
    "    def __call__(self, sent):\n",
    "        postags=['NNP', 'NNG', 'VV', 'VA', 'SL', 'VV+ETN']\n",
    "        pos = self.tagger.pos(sent)\n",
    "        pos = [word for (word, pos) in mc.pos(sent, flatten=True) if pos in postags and len(word)>1]\n",
    "        pos = [word for word in pos if word not in stwlist]\n",
    "        return pos\n",
    "\n",
    "my_tokenizer = MyTokenizer(Mecab(dicpath='The path of the MeCab-ko dictionary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e524de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dldefect_df = pd.read_csv(path+filename, encoding='utf-8')\n",
    "rawComplaints = dldefect_df.apply(lambda row: \" \".join(re.sub(\"[^a-zA-Z가-힣]+\",\" \", str(row.complaint)).split()), 1).to_list()  \n",
    "dldefect_df['complaint'] = rawComplaints\n",
    "dldefect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f096f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#doc vectorization\n",
    "vectorizer = CountVectorizer(max_features=1000, binary=False,\n",
    "                             ngram_range =(1,1), tokenizer= my_tokenizer, stop_words =stwlist)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(rawComplaints) \n",
    "doc_word = ss.csr_matrix(doc_word)\n",
    "\n",
    "vectorizer.vocabulary_\n",
    "idx2vocab = [vocab for vocab, idx in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])]\n",
    "dft_words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "not_digit_inds = [ind for ind, word in enumerate(dft_words) if not word.isdigit()]\n",
    "doc_word = doc_word[:,not_digit_inds]\n",
    "words    = [word for ind,word in enumerate(dft_words) if not word.isdigit()]\n",
    "\n",
    "print(\"doc_word's shape: \",doc_word.shape)\n",
    "print(\"length of idx2vocab: \",len(idx2vocab), idx2vocab[:10])\n",
    "print(\"len of dft_words\",len(dft_words))\n",
    "print(\"len of not_digit_inds\",len(not_digit_inds))\n",
    "print(\"len of words: \",len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label_all = list(dldefect_df['label']) \n",
    "y_labels = list(set(y_label_all))\n",
    "print(y_labels)\n",
    "\n",
    "y_label_dict = {(i):y_labels[i] for i in range(0,len(y_labels))}\n",
    "\n",
    "y_label_encoded=[]\n",
    "for label in y_labels: \n",
    "    for k, v in zip(y_label_dict.keys(), y_label_dict.values()):\n",
    "        if label == v:\n",
    "            y_label_encoded.append(k)\n",
    "            \n",
    "print(len(y_labels), len(y_label_encoded))  \n",
    "print(y_label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mutual_info_classif(doc_word, y_label_all, discrete_features='auto')\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8181048",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = dict(zip(vectorizer.get_feature_names(), \n",
    "                   mutual_info_classif(doc_word, y_label_all, discrete_features='auto')))\n",
    "\n",
    "sorting = sorted(results.items(), key = lambda x : x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "aw =[]\n",
    "miscore=[]\n",
    "for k, v in enumerate(sorting):\n",
    "    aw.append(v[0])\n",
    "    miscore.append(v[1])\n",
    "    \n",
    "awDF = pd.DataFrame({'words':aw, 'mi_score': miscore})\n",
    "\n",
    "awDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5796a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
