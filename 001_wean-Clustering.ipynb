{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53edfe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open files\n",
    "mode = 'B'\n",
    "path = 'dataset path'\n",
    "filename ='dataset file *.csv'\n",
    "stwfile = 'stopwords file *.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07018da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "import MeCab\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab \n",
    "from konlpy.tag import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "from scipy.sparse import diags\n",
    "\n",
    "mc = Mecab(dicpath='The path of the MeCab-ko dictionary.') # The path of the MeCab-ko dictionary.\n",
    "\n",
    "#load stopwords\n",
    "stwDF = pd.read_excel(path+stwfile, sheet_name='stopwords')\n",
    "stwlist = stwDF.space.to_list()\n",
    "\n",
    "class MyTokenizer:\n",
    "    \n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "    \n",
    "    def __call__(self, sent):\n",
    "        postags=['NNP', 'NNG', 'VV', 'VA', 'SL', 'VV+ETN'] \n",
    "        pos = self.tagger.pos(sent)\n",
    "        pos = [word for (word, pos) in mc.pos(sent, flatten=True) if pos in postags and len(word)>1]\n",
    "        pos = [word for word in pos if word not in stwlist]\n",
    "        return pos\n",
    "\n",
    "my_tokenizer = MyTokenizer(Mecab(dicpath='C:/mecab/mecab-ko-dic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae37187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "readfile = pd.read_csv (path+filename, encoding='utf-8')\n",
    "tokenized_input = [' '.join(my_tokenizer(line)) for line in readfile['complaint']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a584cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and embedding both Criteria name and its keywords \n",
    "\n",
    "criteriaDF = pd.read_excel(path+stwfile, sheet_name=mode)\n",
    "criteriaDict = criteriaDF.to_dict()\n",
    "criteria_name = list(criteriaDict.keys())\n",
    "print(\"criteria_name: \",criteria_name)\n",
    "\n",
    "criteria_keywords=[]\n",
    "for k in criteriaDict.values():\n",
    "    temp =[ word for word in list(k.values()) if type(word) is not float]\n",
    "    criteria_keywords.append(temp)\n",
    "# print(\"criteria_keywords: \",criteria_keywords)\n",
    "\n",
    "# class name + keywords \n",
    "input_className=[]\n",
    "for k, v in zip(criteria_name, criteria_keywords): \n",
    "    v.append(k)\n",
    "    input_className.append(' '.join(v))\n",
    "    \n",
    "print(\"Anchor word list: \", input_className)\n",
    "print(len(input_className))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4224e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "criteria_num = list(range(0, len(criteria_name)))\n",
    "print(criteria_num)\n",
    "\n",
    "encoded_dict= {}\n",
    "for k, v in zip(criteria_name, criteria_num):\n",
    "    encoded_dict[k] = v\n",
    "\n",
    "y_label = readfile['label'].to_list()\n",
    "\n",
    "encoded_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a95f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count- vectorization \n",
    "#min_df = minimum number of document that specific word appear : 특정 단어가  n 개 문서 이하에서 등장하면 제외, \n",
    "# analyzer='char' \n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, binary=False, \n",
    "                             ngram_range =(1,1), tokenizer= my_tokenizer, stop_words =stwlist)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(tokenized_input) \n",
    "\n",
    "#csr_matrix: numpy array  > a compressed sparse row matrix로 변환 \n",
    "doc_word = ss.csr_matrix(doc_word)\n",
    "\n",
    "vectorizer.vocabulary_\n",
    "idx2vocab = [vocab for vocab, idx in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])]\n",
    "\n",
    "dft_words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "not_digit_inds = [ind for ind, word in enumerate(dft_words) if not word.isdigit()]\n",
    "doc_word = doc_word[:,not_digit_inds]\n",
    "words    = [word for ind,word in enumerate(dft_words) if not word.isdigit()]\n",
    "\n",
    "\n",
    "print(\"doc_word's shape: \",doc_word.shape)\n",
    "print(\"length of idx2vocab: \",len(idx2vocab), idx2vocab[:10])\n",
    "print(\"len of dft_words\",len(dft_words))\n",
    "print(\"len of not_digit_inds\",len(not_digit_inds))\n",
    "print(\"len of words: \",len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfda1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_className)\n",
    "#print anchor words\n",
    "\n",
    "awlist= []\n",
    "for aw in input_className:   #criteria_name input_className\n",
    "    awlist.append(my_tokenizer(aw))\n",
    "print(awlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight calculation\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "results = dict(zip(vectorizer.get_feature_names(), \n",
    "                   mutual_info_classif(doc_word, awlist, discrete_features='auto')))\n",
    "\n",
    "sorting = sorted(results.items(), key = lambda x : x[1], reverse = True)\n",
    "aw =[]\n",
    "miscore=[]\n",
    "for k, v in enumerate(sorting):\n",
    "    aw.append(v[0])\n",
    "    miscore.append(v[1])\n",
    "    \n",
    "\n",
    "# aw_weight ={}\n",
    "# for i in range(len(awlist)): \n",
    "#     for j in range(len(awlist[i])):\n",
    "#         aw_weight[awlist[i][j]]= aw_w\n",
    "# print(aw_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = doc_word.toarray()\n",
    "print(matrix.shape)\n",
    "#get_feature_names() \n",
    "dft_words_weight = np.ones(len(words))\n",
    "\n",
    "for key, value in aw_weight.items():\n",
    "    if key in words:\n",
    "        dft_words_weight[words.index(key)]= value\n",
    "\n",
    "print(dft_words_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a286c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_diag = diags(dft_words_weight, 0)\n",
    "print(weight_diag.shape)\n",
    "weight_diag = diags(dft_words_weight, 0)\n",
    "print(weight_diag.shape)\n",
    "\n",
    "np_weight_diag = weight_diag.toarray()\n",
    "print(np_weight_diag.shape)\n",
    "\n",
    "\n",
    "doc_word_w = ss.csr_matrix(np.matmul(matrix,np_weight_diag))\n",
    "doc_word_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e78a8",
   "metadata": {},
   "source": [
    "\n",
    "## CorEx Topic Model\n",
    "The main parameters of the CorEx topic model are:\n",
    "\n",
    "- n_hidden: number of topics (\"hidden\" as in \"hidden latent topics\")\n",
    "- words: words that label the columns of the doc-word matrix (optional)\n",
    "- docs: document labels that label the rows of the doc-word matrix (optional)\n",
    "- max_iter: number of iterations to run through the update equations (optional, defaults to 200)\n",
    "- verbose: if verbose=1, then CorEx will print the topic TCs with each iteration\n",
    "- seed: random number seed to use for model initialization (optional)\n",
    "\n",
    "As shown in the example, \n",
    "- clusters gives the variable clusters for each hidden factor Y_j and \n",
    "- labels gives the labels for each sample for each Y_j. \n",
    "- Probabilistic labels can be accessed with p_y_given_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acff630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CorEx-AnchorWords run\n",
    "\"\"\"\n",
    "Choosing anchor strength: \n",
    "the anchor strength controls how much weight CorEx puts towards maximizing the mutual information \n",
    "between the anchor words and their respective topics. \n",
    "Anchor strength should always be set at a value greater than 1, \n",
    "since setting anchor strength between 0 and 1 only recovers the unsupervised CorEx objective. \n",
    "Empirically, setting anchor strength from 1.5-3 seems to nudge the topic model towards the anchor words. \n",
    "Setting anchor strength greater than 5 is strongly enforcing that the CorEx topic model find \n",
    "a topic associated with the anchor words.\n",
    "\n",
    "We encourage users to experiment with the anchor strength and determine what values are best for their needs.\n",
    "\"\"\"\n",
    "\n",
    "import corextopic.corextopic as ct\n",
    "import corextopic.vis_topic as vt \n",
    "\n",
    "criteria_num = len(awlist) #awlist\n",
    "anchor_words = awlist  #awlist\n",
    "anchor_strength_v = 10\n",
    "\n",
    "anchoring_model= ct.Corex(n_hidden =criteria_num, seed=37) \n",
    "anchoring_model.fit(doc_word_w, \n",
    "                    words=words,\n",
    "                    anchors= anchor_words, \n",
    "                    anchor_strength=anchor_strength_v);\n",
    "\n",
    "\n",
    "#Overall TC : Topic Correlation and Model selection :\n",
    "print(anchoring_model.tcs.shape) # k_topics\n",
    "print(np.sum(anchoring_model.tcs))\n",
    "print(\"The overall total correlation is the sum of the total correlation per each topic.\")\n",
    "print(\"TC value= \", anchoring_model.tc)\n",
    "print(\"For an anchored CorEx topic model, the topics are not sorted, and are outputted such that the anchored topics come first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7a165",
   "metadata": {},
   "source": [
    "### OUTPUT of CorEx\n",
    "\n",
    "- text_files/groups.txt Lists the variables in each group.\n",
    "\n",
    "- text_files/labels.txt Gives a column for each latent factor (in layer 1) and a row for each patient/sample. The entry is the value of the latent factor (0,…dim_hidden-1)\n",
    "\n",
    "- text_files/cont_labels.txt Gives a continuous number to sort each patient with respect to each latent factor.\n",
    "\n",
    "- relationships For each latent factor, it shows pairwise plots between the top genes in each group. Each point corresponds to a sample/patient and the color corresponds to the learned latent factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " CorEx Attributes\n",
    "    ----------\n",
    "    \n",
    "    labels : array, [n_samples, n_hidden]\n",
    "        Label for each hidden unit for each sample.\n",
    "\n",
    "    clusters : array, [n_visible]\n",
    "        Cluster label for each input variable.\n",
    "\n",
    "    p_y_given_x : array, [n_samples, n_hidden]\n",
    "        p(y_j=1|x) for each sample.\n",
    "\n",
    "    alpha : array-like, shape [n_hidden, n_visible]\n",
    "        Adjacency matrix between input variables and hidden units. In range [0,1].\n",
    "\n",
    "    mis : array, [n_hidden, n_visible]\n",
    "        Mutual information between each (visible/observed) variable and hidden unit\n",
    "\n",
    "    tcs : array, [n_hidden]\n",
    "        X_G = a group of word types \n",
    "        Y = a topic to be learned \n",
    "        corex는 TC(X_Gj;Y_j) 를 maximize 하여 latent topics를 통해 문서내 단어의 종속성을 최대한 설명하려고 함 \n",
    "        TC(X_Gj;Y_j) for each hidden unit\n",
    "\n",
    "    tc : float\n",
    "        Convenience variable = Sum_j tcs[j]\n",
    "\n",
    "    tc_history : array\n",
    "        Shows value of TC over the course of learning. Hopefully, it is converging.\n",
    "\n",
    "    words : list of strings\n",
    "        Feature names that label the corresponding columns of X\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tcs = anchoring_model.tcs\n",
    "mis = anchoring_model.mis \n",
    "sign = anchoring_model.sign\n",
    "alpha = anchoring_model.alpha\n",
    "resLabels = anchoring_model.labels\n",
    "p_y_given_x = anchoring_model.p_y_given_x\n",
    "ac_topics = anchoring_model.get_topics()\n",
    "row_label = list(map(str, range(anchoring_model.n_samples)))\n",
    "\n",
    "print(\"tcs's shape:\" ,tcs.shape)\n",
    "print(\"mis's shape:\" ,mis.shape)\n",
    "print(\"sign's shape:\" ,sign.shape)\n",
    "print(\"alpha's shape:\" ,alpha.shape)\n",
    "print(\"resLabels'shape:\", resLabels.shape)\n",
    "print(\"p_y_given_x's shape:\" ,p_y_given_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f0cfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " topics, list or list of lists\n",
    "            Each list is a topic. If only topic is being queried, then only a\n",
    "            single, non-nested list. Each topic list contains a series of\n",
    "            3-tuples for the top N words. \n",
    "            1) The first entry is either the string or the column integer index of the word, depending on `print_words`\n",
    "            and whether `words` is available. \n",
    "            2) The second entry is the mutual information (MI) of the word with the topic. \n",
    "            3) The third entry is the sign of correlation of the word with the topic. If it is\n",
    "            positive (1), then the word's presence is informative for the topic.\n",
    "            If it is negative (-1), then word's absensce is informative for the\n",
    "            topic\n",
    "\"\"\"\n",
    "getTopicWordlist={}\n",
    "for n,topic in enumerate(ac_topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    getTopicWordlist[n]= topic_words\n",
    "    print('{}: '.format(n) + ', '.join(topic_words))\n",
    "    \n",
    "twDF = pd.DataFrame({'topicTemrs': list(getTopicWordlist.values()), 'tc': tcs})\n",
    "\n",
    "twDF['ct']= criteria_name\n",
    "\n",
    "\n",
    "print(encoded_dict)\n",
    "\n",
    "twDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac370a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maxPlabel=[]\n",
    "ns, m = p_y_given_x.shape\n",
    "print(ns, m)\n",
    "\n",
    "for l in range(ns):\n",
    "    temp = list(map(lambda q: '{:.3f}'.format(q),list(np.log(p_y_given_x[l, :]))))\n",
    "    temp = list(map(float,temp))\n",
    "    maxPlabel.append(temp.index(max(temp)))\n",
    "\n",
    "print(\"Selected label or each sample\",maxPlabel[:10])\n",
    "print(len(maxPlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb703f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_str=[]\n",
    "for pred_y_idx in maxPlabel:\n",
    "    for k,v in encoded_dict.items():\n",
    "        if pred_y_idx== v: \n",
    "            pred_y_str.append(k)\n",
    "print(len(pred_y_str))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161c386",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reClassifyingDF =pd.DataFrame({'complaint': tokenized_input, 'y_pred':pred_y_str, 'y_ans_str': y_label})\n",
    "reClassifyingDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e87e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "print(classification_report(y_label, pred_y_str))\n",
    "print(precision_recall_fscore_support(y_label, pred_y_str,  average='micro'))\n",
    "print(\"TC value= \", anchoring_model.tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71466bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_name_A =['Cabinet', \n",
    "                    'Gas and fire system', \n",
    "                    'Condensation',\n",
    "                    'Structural defect',\n",
    "                    'Heating system',\n",
    "                    'Leakage',\n",
    "                    'Paperwork',\n",
    "                    'Woodframe work',\n",
    "                    'Masonry',\n",
    "                    'Flooring',\n",
    "                    'Stonework',\n",
    "                    'Finish of sink',\n",
    "                    'Airconditioning system',\n",
    "                    'Appliance',\n",
    "                    'Sanitary and plumbing',\n",
    "                    'Electrical system',\n",
    "                    'Doors and windows',\n",
    "                    'Tiling',\n",
    "                    'Communication system']\n",
    "\n",
    "criteria_name_B=['Opening', 'Condensation', 'Stability', 'Leakage', 'Step', 'Detachement', 'Poor surface', 'Uninstallation', \n",
    "                  'Corrosion', 'Misalignment', 'Contamination', 'Disconnection', 'Out of orders', 'On/Off defect', 'Poor joint', \n",
    "                  'Scratch', 'Caulking defect', 'Crack', 'Broken']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ba992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # used for plot interactive graph.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "font = {'family' : 'Malgun Gothic',\n",
    "        'size'   : 12}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# plt.rc('font', family='Malgun Gothic')\n",
    "\n",
    "conf_mat = confusion_matrix(y_label, pred_y_str)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d', annot_kws={\"size\": 10},\n",
    "            xticklabels=criteria_name_B, \n",
    "            yticklabels=criteria_name_B)\n",
    "plt.ylabel('True Label', size=12)\n",
    "plt.xlabel('Predicted Label', size=12)\n",
    "plt.title(\"Confusion Matrix \", size=15);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db98c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_ans_dict = {i: criteria_name[i] for i in range(0, len(criteria_name))}\n",
    "print(y_ans_dict)\n",
    "\n",
    "ans_idx = []\n",
    "for anslabel in y_label:\n",
    "    for k, v in y_ans_dict.items():\n",
    "        if anslabel == v:\n",
    "            ans_idx.append(k)\n",
    "            \n",
    "reClassifyingDF['y_ans_idx'] = ans_idx \n",
    "reClassifyingDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd6fea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ans_idx= np.array(ans_idx).reshape(-1,1)\n",
    "y_pred = maxPlabel\n",
    "y_pred = np.array(y_pred).reshape(-1,1)\n",
    "print(type(ans_idx))\n",
    "\n",
    "\n",
    "score_samples = silhouette_samples(p_y_given_x, y_pred)\n",
    "print(score_samples.shape)\n",
    "print(score_samples)\n",
    "\n",
    "reClassifyingDF['silhoutte_coeff'] = score_samples\n",
    "\n",
    "print('silhouette Coefficient: {:.4f}'.format(silhouette_score(p_y_given_x, y_pred)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
